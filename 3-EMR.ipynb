{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EMR, HIVE, PRESTO and SPARK\n",
    "===\n",
    "\n",
    "**Creation of EMR CLUSTER.**\n",
    "\n",
    "We create a cluster with Hive + Pig + Presto + Hue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparation\n",
    "====\n",
    "Before creating the EMR cluster, we will configure the Environment Variables **VPCSGID** and **SUBNET** with the following command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import boto3,os\n",
    "cf = boto3.client('cloudformation', region_name='eu-west-1')\n",
    "response = cf.describe_stacks(StackName=os.getenv('STACKNAME'))\n",
    "outputs = response['Stacks'][0]['Outputs']\n",
    "for output in outputs:\n",
    "    if output['OutputKey'] == 'VPCSGID':\n",
    "        os.environ['VPCSGID']=output['OutputValue']\n",
    "    if output['OutputKey'] == 'Subnet':\n",
    "        os.environ['SUBNET']=output['OutputValue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%env BUCKET=aws-potus-eu-west-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!aws ec2 create-key-pair --key-name $USER$WORKSHOP --query 'KeyMaterial' --output text --region eu-west-1 \\\n",
    "   > $USER$WORKSHOP.pem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod 500 $USER$WORKSHOP.pem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!aws emr create-default-roles --region eu-west-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!aws emr create-cluster --release-label emr-5.11.0 --name \"EMR cluster\" \\\n",
    "  --applications Name=Hadoop Name=Hue Name=Spark Name=Hive Name=Zeppelin Name=HCatalog Name=Presto \\\n",
    "  --ec2-attributes KeyName=$USER$WORKSHOP,SubnetId=$SUBNET,AdditionalMasterSecurityGroups=$VPCSGID --use-default-roles \\\n",
    "  --instance-groups InstanceGroupType=MASTER,InstanceCount=1,InstanceType=m4.xlarge \\\n",
    "    InstanceGroupType=CORE,InstanceCount=1,InstanceType=m4.xlarge \\\n",
    "  --region eu-west-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%env CLUSTER="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!aws emr describe-cluster --cluster-id $CLUSTER --region eu-west-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Usage of EMR CLUSTER with pyhive**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create the connection strings, then create a table with HIVE with data on S3, and then do a sample request with HIVE and then PRESTO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyhive import hive\n",
    "from pyhive import presto\n",
    "import os\n",
    "\n",
    "hive_conn = hive.Connection(host=\"IP-OF-MASTER-EMR\",\\\n",
    "                            configuration={'hive.execution.engine':'mr'},port=10000)\n",
    "\n",
    "presto_conn = presto.Connection(host=\"IP-OF-MASTER-EMR\", port=8889)\n",
    "\n",
    "bucket = \"s3://\"+os.getenv('USER')+\"-\"+os.getenv('WORKSHOP')+\"-aws-bigdata-workshop/split/Hillary/\"\n",
    "createclinton = \"CREATE EXTERNAL TABLE hillary (id BIGINT,name STRING,text STRING,time BIGINT,isodate TIMESTAMP)\\\n",
    "    ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' LINES TERMINATED BY '\\n' \\\n",
    "    LOCATION '\"+bucket+\"'\"\n",
    "\n",
    "bucket = \"s3://\"+os.getenv('USER')+\"-\"+os.getenv('WORKSHOP')+\"-aws-bigdata-workshop/split/Donald/\"\n",
    "createtrump = \"CREATE EXTERNAL TABLE donald (id BIGINT,name STRING,text STRING,time BIGINT,isodate TIMESTAMP)\\\n",
    "    ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' LINES TERMINATED BY '\\n' \\\n",
    "    LOCATION '\"+bucket+\"'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hive_cursor=hive_conn.cursor()\n",
    "hive_cursor.execute(createtrump)\n",
    "hive_cursor.execute(createclinton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hive_cursor=hive_conn.cursor()\n",
    "hive_cursor.execute(\"select count(*) from hillary\")\n",
    "print(hive_cursor.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "presto_cursor=presto_conn.cursor()\n",
    "presto_cursor.execute('SELECT count(*) FROM hillary')\n",
    "print(presto_cursor.fetchall())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other sample requests to test :\n",
    "```\n",
    "select count(*) from hillary\n",
    "select count(distinct id) from hillary\n",
    "select count(distinct id) from hillary where text like '%obama%'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SPARK with zeppelin**\n",
    "\n",
    "Connect on http://EMR-MASTER-IP:8890/\n",
    "Create a new Spark Notebook\n",
    "We will use our previously created hive tables.\n",
    "\n",
    "COPY/PASTE these code-blocks.\n",
    "\n",
    "ANALYZE DATA WITH SPARK SQL :\n",
    "```\n",
    "%sql\n",
    "select date_sub(isodate, 1) as date,'clinton' as candidate,count(distinct id) as count from hillary group by 1 union select date_sub(isodate, 1) as date,'trump' as candidate,count(distinct id) as count from donald group by 1 order by date\n",
    "```\n",
    "\n",
    "Do sentiment analysis with SPARK\n",
    "```\n",
    "import org.apache.spark.sql.DataFrame\n",
    "\n",
    "val af = sc.textFile(\"s3://aws-potus-eu-west-1/sample/AFINN-en-165.txt\").map(x=> x.split(\"\\t\")).map(x=>(x(0).toString,x(1).toInt)).collectAsMap()\n",
    "case class Tweet(id: String, name: String,message: String, time: String, isodate : String,score: Integer)\n",
    "\n",
    "def sentiment(candidate:String) : DataFrame= {         \n",
    "    val x = sc.textFile(\"s3://aws-potus-eu-west-1/split/\"+candidate+\"/*\").map(_.split(\"\\\\|\")).flatMap{\n",
    "        row =>\n",
    "            if (row.length==5){\n",
    "                  var s = row(2).toString.split(\" \").map(word => {\n",
    "                    var senti: Int = 0\n",
    "                    if (af!=None && word!=None && af.get(word.toLowerCase())!=None){\n",
    "                        senti =af.get (word.toLowerCase()).get    \n",
    "                    }\n",
    "                    senti;\n",
    "                })\n",
    "                Some(Tweet(row(0),row(1),row(2),row(3),row(4),s.sum))\n",
    "            }else{\n",
    "                None\n",
    "            }\n",
    "    }\n",
    "    return x.toDF()\n",
    "}\n",
    "val trump = sentiment(\"Donald\")\n",
    "trump.createOrReplaceTempView(\"donaldwithsentiment\")\n",
    "val clinton = sentiment(\"Hillary\")\n",
    "clinton.createOrReplaceTempView(\"hillarywithsentiment\")\n",
    "```\n",
    "\n",
    "ANALYZE WITH SPARK SQL and sentiment\n",
    "```\n",
    "%sql\n",
    "select date_sub(isodate, 1) as date,'clinton' as candidate,count(distinct id) as count,sum(score) from hillarywithsentiment group by 1 union select date_sub(isodate, 1) as date,'trump' as candidate,count(distinct id) as count,sum(score) from donaldwithsentiment group by 1 order by date\n",
    "```\n",
    "\n",
    "\n",
    "ALTERNATIVE REQUEST SPARK SQL programmatically\n",
    "```\n",
    "val sqlContext = new org.apache.spark.sql.hive.HiveContext(sc)\n",
    "val result = sqlContext.sql(\"SELECT count(*) from hillary\")\n",
    "result.show()\n",
    "val df = sqlContext.sql(\"SELECT * from hillary\")\n",
    "df.printSchema()\n",
    "df.groupBy(\"name\").count().show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Usage of EMR CLUSTER with EMR steps**\n",
    "\n",
    "Can be used for PIG, HIVE or JAVA/SPARK activities.\n",
    "\n",
    "For the workshop we will use Hive steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!aws s3 cp s3://$BUCKET/sample/hillary.q . --only-show-errors\n",
    "!cat hillary.q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!aws emr add-steps --region eu-west-1 --cluster-id $CLUSTER\\\n",
    "  --steps Type=HIVE,Name='Hive program',ActionOnFailure=CONTINUE,Args=[-f,s3://$BUCKET/sample/hillary.q,-d,INPUT=s3://$BUCKET/sample/,-d,OUTPUT=s3://$BUCKET/sample,arg1,arg2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!aws emr list-steps --cluster-id $CLUSTER --region eu-west-1|jq '.Steps[0]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!aws s3 cp s3://$BUCKET/sample/select-count-hillary.q . --only-show-errors\n",
    "!cat select-count-hillary.q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!aws emr add-steps --region eu-west-1 --cluster-id $CLUSTER\\\n",
    "  --steps Type=HIVE,Name='Hive program',ActionOnFailure=CONTINUE,Args=[-f,s3://$BUCKET/sample/select-count-hillary.q,-d,INPUT=s3://$BUCKET/sample/,-d,OUTPUT=s3://$BUCKET/sample,arg1,arg2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!aws emr list-steps --cluster-id $CLUSTER --region eu-west-1|jq '.Steps[0]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!aws s3 ls s3://$BUCKET/sample/output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!aws s3 cp s3://$BUCKET/sample/output/000000_0 .\n",
    "!cat 000000_0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
